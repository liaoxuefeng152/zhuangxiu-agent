#!/usr/bin/env python3
"""
è£…ä¿®é¿å‘ç®¡å®¶ - å…¨é¢æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ç±»å‹ï¼šAPIæ€§èƒ½åŸºå‡†æµ‹è¯•ã€å¹¶å‘æ€§èƒ½æµ‹è¯•ã€è´Ÿè½½æµ‹è¯•
"""
import requests
import time
import json
import threading
import statistics
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_URL = "http://120.26.201.61:8001/api/v1"

class Colors:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'

def print_success(msg):
    print(f"{Colors.GREEN}âœ… {msg}{Colors.RESET}")

def print_warning(msg):
    print(f"{Colors.YELLOW}âš ï¸  {msg}{Colors.RESET}")

def print_error(msg):
    print(f"{Colors.RED}âŒ {msg}{Colors.RESET}")

def print_info(msg):
    print(f"{Colors.BLUE}â„¹ï¸  {msg}{Colors.RESET}")

def print_header(title):
    print(f"\n{Colors.BOLD}{Colors.BLUE}{'='*80}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{title}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{'='*80}{Colors.RESET}\n")

class PerformanceTester:
    def __init__(self):
        self.token = None
        self.user_id = None
        self.results = {}
        
    def login(self):
        """è·å–æµ‹è¯•token"""
        print_info("è·å–æµ‹è¯•token...")
        try:
            resp = requests.post(
                f"{BASE_URL}/users/login",
                json={"code": "dev_weapp_mock"},
                timeout=10
            )
            resp.raise_for_status()
            result = resp.json()
            
            if result.get("code") == 0:
                data = result.get("data", {})
            else:
                data = result
            
            self.token = data.get("access_token")
            self.user_id = data.get("user_id")
            
            if not self.token:
                print_error("ç™»å½•å¤±è´¥ï¼šæœªè·å–åˆ°token")
                return False
            
            print_success(f"ç™»å½•æˆåŠŸ (User ID: {self.user_id})")
            return True
        except Exception as e:
            print_error(f"ç™»å½•å¤±è´¥: {e}")
            return False
    
    def get_headers(self):
        """è·å–è¯·æ±‚å¤´"""
        if not self.token:
            return {}
        return {
            "Authorization": f"Bearer {self.token}",
            "X-User-Id": str(self.user_id)
        }
    
    def test_single_api(self, name, method, endpoint, data=None, params=None):
        """æµ‹è¯•å•ä¸ªAPIçš„æ€§èƒ½"""
        print_info(f"æµ‹è¯• {name}...")
        
        url = f"{BASE_URL}/{endpoint}"
        headers = self.get_headers()
        
        times = []
        errors = 0
        
        # æµ‹è¯•5æ¬¡å–å¹³å‡å€¼
        for i in range(5):
            try:
                start_time = time.time()
                
                if method == "GET":
                    resp = requests.get(url, headers=headers, params=params, timeout=10)
                elif method == "POST":
                    resp = requests.post(url, headers=headers, json=data, timeout=10)
                elif method == "PUT":
                    resp = requests.put(url, headers=headers, json=data, timeout=10)
                elif method == "DELETE":
                    resp = requests.delete(url, headers=headers, timeout=10)
                else:
                    print_error(f"ä¸æ”¯æŒçš„HTTPæ–¹æ³•: {method}")
                    return None
                
                elapsed = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                times.append(elapsed)
                
                if resp.status_code >= 400:
                    print_warning(f"  {name} ç¬¬{i+1}æ¬¡è¯·æ±‚è¿”å› {resp.status_code}")
                    errors += 1
                
            except Exception as e:
                print_warning(f"  {name} ç¬¬{i+1}æ¬¡è¯·æ±‚å¤±è´¥: {e}")
                errors += 1
        
        if not times:
            print_error(f"{name} æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥")
            return None
        
        result = {
            "name": name,
            "method": method,
            "endpoint": endpoint,
            "avg_time_ms": statistics.mean(times),
            "min_time_ms": min(times),
            "max_time_ms": max(times),
            "std_dev_ms": statistics.stdev(times) if len(times) > 1 else 0,
            "errors": errors,
            "success_rate": ((5 - errors) / 5) * 100
        }
        
        status = "âœ…" if result["success_rate"] >= 80 and result["avg_time_ms"] < 1000 else "âš ï¸" if result["success_rate"] >= 50 else "âŒ"
        print(f"  {status} {name}: å¹³å‡ {result['avg_time_ms']:.2f}ms, æˆåŠŸç‡ {result['success_rate']:.1f}%")
        
        return result
    
    def test_concurrent_requests(self, endpoint, num_requests=10, num_threads=5):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        print_info(f"æµ‹è¯•å¹¶å‘è¯·æ±‚: {endpoint} (è¯·æ±‚æ•°: {num_requests}, çº¿ç¨‹æ•°: {num_threads})")
        
        url = f"{BASE_URL}/{endpoint}"
        headers = self.get_headers()
        
        times = []
        errors = 0
        lock = threading.Lock()
        
        def make_request(i):
            try:
                start_time = time.time()
                resp = requests.get(url, headers=headers, timeout=10)
                elapsed = (time.time() - start_time) * 1000
                
                with lock:
                    times.append(elapsed)
                    if resp.status_code >= 400:
                        return False
                return True
            except Exception as e:
                with lock:
                    errors += 1
                return False
        
        start_total = time.time()
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(make_request, i) for i in range(num_requests)]
            results = [f.result() for f in as_completed(futures)]
        
        total_time = (time.time() - start_total) * 1000
        
        if not times:
            print_error("æ‰€æœ‰å¹¶å‘è¯·æ±‚éƒ½å¤±è´¥")
            return None
        
        result = {
            "endpoint": endpoint,
            "num_requests": num_requests,
            "num_threads": num_threads,
            "total_time_ms": total_time,
            "avg_time_ms": statistics.mean(times),
            "min_time_ms": min(times),
            "max_time_ms": max(times),
            "requests_per_second": num_requests / (total_time / 1000),
            "errors": errors,
            "success_rate": ((num_requests - errors) / num_requests) * 100
        }
        
        print(f"  ğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ: {result['requests_per_second']:.2f} è¯·æ±‚/ç§’, æˆåŠŸç‡ {result['success_rate']:.1f}%")
        
        return result
    
    def test_load_scenario(self, scenario_name, duration_seconds=30, requests_per_second=5):
        """æµ‹è¯•è´Ÿè½½åœºæ™¯"""
        print_info(f"æµ‹è¯•è´Ÿè½½åœºæ™¯: {scenario_name} (æŒç»­æ—¶é—´: {duration_seconds}ç§’, è¯·æ±‚é¢‘ç‡: {requests_per_second}/ç§’)")
        
        # æµ‹è¯•å¤šä¸ªAPIçš„æ··åˆè´Ÿè½½
        endpoints = [
            ("users/profile", "GET"),
            ("constructions/schedule", "GET"),
            ("companies/scans", "GET"),
            ("quotes/list", "GET"),
        ]
        
        headers = self.get_headers()
        
        total_requests = 0
        successful_requests = 0
        response_times = []
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        print_info("å¼€å§‹è´Ÿè½½æµ‹è¯•...")
        
        while time.time() < end_time:
            batch_start = time.time()
            
            for endpoint, method in endpoints:
                if time.time() >= end_time:
                    break
                    
                try:
                    url = f"{BASE_URL}/{endpoint}"
                    request_start = time.time()
                    
                    if method == "GET":
                        resp = requests.get(url, headers=headers, timeout=5)
                    else:
                        continue
                    
                    elapsed = (time.time() - request_start) * 1000
                    response_times.append(elapsed)
                    total_requests += 1
                    
                    if resp.status_code < 400:
                        successful_requests += 1
                    
                except Exception:
                    total_requests += 1
                    # ç»§ç»­æµ‹è¯•ï¼Œä¸ä¸­æ–­
            
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            batch_elapsed = time.time() - batch_start
            if batch_elapsed < 1.0 / requests_per_second:
                time.sleep(1.0 / requests_per_second - batch_elapsed)
        
        total_elapsed = time.time() - start_time
        
        if not response_times:
            print_error("è´Ÿè½½æµ‹è¯•æ²¡æœ‰æˆåŠŸè¯·æ±‚")
            return None
        
        result = {
            "scenario_name": scenario_name,
            "duration_seconds": duration_seconds,
            "target_rps": requests_per_second,
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "actual_rps": total_requests / total_elapsed,
            "avg_response_time_ms": statistics.mean(response_times) if response_times else 0,
            "p95_response_time_ms": sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0,
            "p99_response_time_ms": sorted(response_times)[int(len(response_times) * 0.99)] if response_times else 0,
            "success_rate": (successful_requests / total_requests * 100) if total_requests > 0 else 0
        }
        
        print(f"  ğŸ“ˆ è´Ÿè½½æµ‹è¯•ç»“æœ: {result['actual_rps']:.2f} å®é™…è¯·æ±‚/ç§’, å¹³å‡å“åº” {result['avg_response_time_ms']:.2f}ms, æˆåŠŸç‡ {result['success_rate']:.1f}%")
        
        return result
    
    def run_api_benchmarks(self):
        """è¿è¡ŒAPIåŸºå‡†æµ‹è¯•"""
        print_header("APIæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        benchmarks = [
            ("å¥åº·æ£€æŸ¥", "GET", "health", None, None),
            ("ç”¨æˆ·ä¿¡æ¯", "GET", "users/profile", None, None),
            ("åŸå¸‚åˆ—è¡¨", "GET", "cities/list", None, None),
            ("çƒ­é—¨åŸå¸‚", "GET", "cities/hot", None, None),
            ("å…¬å¸æ£€æµ‹è®°å½•", "GET", "companies/scans", None, None),
            ("æŠ¥ä»·å•åˆ—è¡¨", "GET", "quotes/list", None, None),
            ("åˆåŒåˆ—è¡¨", "GET", "contracts/list", None, None),
            ("æ–½å·¥è¿›åº¦", "GET", "constructions/schedule", None, None),
            ("æ¶ˆæ¯åˆ—è¡¨", "GET", "messages", None, None),
            ("æ–½å·¥ç…§ç‰‡åˆ—è¡¨", "GET", "construction-photos", None, None),
        ]
        
        results = []
        for benchmark in benchmarks:
            result = self.test_single_api(*benchmark)
            if result:
                results.append(result)
        
        self.results["api_benchmarks"] = results
        return results
    
    def run_concurrency_tests(self):
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        print_header("å¹¶å‘æ€§èƒ½æµ‹è¯•")
        
        concurrency_tests = [
            ("users/profile", 20, 5),
            ("constructions/schedule", 20, 5),
            ("companies/scans", 20, 5),
        ]
        
        results = []
        for endpoint, num_requests, num_threads in concurrency_tests:
            result = self.test_concurrent_requests(endpoint, num_requests, num_threads)
            if result:
                results.append(result)
        
        self.results["concurrency_tests"] = results
        return results
    
    def run_load_tests(self):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        print_header("è´Ÿè½½æµ‹è¯•")
        
        load_scenarios = [
            ("æ­£å¸¸è´Ÿè½½", 30, 5),
            ("ä¸­ç­‰è´Ÿè½½", 30, 10),
            ("é«˜è´Ÿè½½", 30, 20),
        ]
        
        results = []
        for scenario_name, duration, rps in load_scenarios:
            result = self.test_load_scenario(scenario_name, duration, rps)
            if result:
                results.append(result)
        
        self.results["load_tests"] = results
        return results
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"tests/performance_test_report_{timestamp}.md"
        
        report = f"""# è£…ä¿®é¿å‘ç®¡å®¶ - å…¨é¢æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ä¿¡æ¯
- **æµ‹è¯•æ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **æµ‹è¯•ç¯å¢ƒ**: é˜¿é‡Œäº‘å¼€å‘ç¯å¢ƒ (120.26.201.61:8001)
- **Pythonç‰ˆæœ¬**: 3.12.8

## æµ‹è¯•æ¦‚è¿°

æœ¬æ¬¡æ€§èƒ½æµ‹è¯•åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
1. **APIæ€§èƒ½åŸºå‡†æµ‹è¯•** - æµ‹è¯•å•ä¸ªAPIçš„å“åº”æ—¶é—´
2. **å¹¶å‘æ€§èƒ½æµ‹è¯•** - æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„è¡¨ç°
3. **è´Ÿè½½æµ‹è¯•** - æµ‹è¯•ç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°

## 1. APIæ€§èƒ½åŸºå‡†æµ‹è¯•

### æ€§èƒ½è¦æ±‚
- P0 API: å“åº”æ—¶é—´ â‰¤ 500ms
- P1 API: å“åº”æ—¶é—´ â‰¤ 1000ms
- P2 API: å“åº”æ—¶é—´ â‰¤ 1500ms

### æµ‹è¯•ç»“æœ
| APIåç§° | æ–¹æ³• | ç«¯ç‚¹ | å¹³å‡å“åº”æ—¶é—´(ms) | æœ€å°(ms) | æœ€å¤§(ms) | æˆåŠŸç‡ | çŠ¶æ€ |
|---------|------|------|------------------|----------|----------|--------|------|
"""
        
        # APIåŸºå‡†æµ‹è¯•ç»“æœ
        if "api_benchmarks" in self.results:
            for result in self.results["api_benchmarks"]:
                status = "âœ…" if result["success_rate"] >= 80 and result["avg_time_ms"] < 1000 else "âš ï¸" if result["success_rate"] >= 50 else "âŒ"
                report += f"| {result['name']} | {result['method']} | {result['endpoint']} | {result['avg_time_ms']:.2f} | {result['min_time_ms']:.2f} | {result['max_time_ms']:.2f} | {result['success_rate']:.1f}% | {status} |\n"
        
        report += f"""
## 2. å¹¶å‘æ€§èƒ½æµ‹è¯•

### æµ‹è¯•é…ç½®
- å¹¶å‘çº¿ç¨‹æ•°: 5
- æ¯ä¸ªç«¯ç‚¹è¯·æ±‚æ•°: 20

### æµ‹è¯•ç»“æœ
| ç«¯ç‚¹ | æ€»è¯·æ±‚æ•° | æˆåŠŸè¯·æ±‚æ•° | æˆåŠŸç‡ | æ€»æ—¶é—´(ms) | å¹³å‡å“åº”æ—¶é—´(ms) | ååé‡(è¯·æ±‚/ç§’) | çŠ¶æ€ |
|------|----------|------------|--------|------------|------------------|-----------------|------|
"""
        
        # å¹¶å‘æµ‹è¯•ç»“æœ
        if "concurrency_tests" in self.results:
            for result in self.results["concurrency_tests"]:
                status = "âœ…" if result["success_rate"] >= 90 else "âš ï¸" if result["success_rate"] >= 70 else "âŒ"
                report += f"| {result['endpoint']} | {result['num_requests']} | {result['num_requests'] - result['errors']} | {result['success_rate']:.1f}% | {result['total_time_ms']:.2f} | {result['avg_time_ms']:.2f} | {result['requests_per_second']:.2f} | {status} |\n"
        
        report += f"""
## 3. è´Ÿè½½æµ‹è¯•

### æµ‹è¯•åœºæ™¯
1. **æ­£å¸¸è´Ÿè½½**: 5è¯·æ±‚/ç§’ï¼ŒæŒç»­30ç§’
2. **ä¸­ç­‰è´Ÿè½½**: 10è¯·æ±‚/ç§’ï¼ŒæŒç»­30ç§’
3. **é«˜è´Ÿè½½**: 20è¯·æ±‚/ç§’ï¼ŒæŒç»­30ç§’

### æµ‹è¯•ç»“æœ
| åœºæ™¯åç§° | ç›®æ ‡RPS | å®é™…RPS | æ€»è¯·æ±‚æ•° | æˆåŠŸè¯·æ±‚æ•° | æˆåŠŸç‡ | å¹³å‡å“åº”æ—¶é—´(ms) | P95å“åº”æ—¶é—´(ms) | P99å“åº”æ—¶é—´(ms) | çŠ¶æ€ |
|----------|---------|---------|----------|------------|--------|------------------|-----------------|-----------------|------|
"""
        
        # è´Ÿè½½æµ‹è¯•ç»“æœ
        if "load_tests" in self.results:
            for result in self.results["load_tests"]:
                status = "âœ…" if result["success_rate"] >= 95 else "âš ï¸" if result["success_rate"] >= 80 else "âŒ"
                report += f"| {result['scenario_name']} | {result['target_rps']} | {result['actual_rps']:.2f} | {result['total_requests']} | {result['successful_requests']} | {result['success_rate']:.1f}% | {result['avg_response_time_ms']:.2f} | {result['p95_response_time_ms']:.2f} | {result['p99_response_time_ms']:.2f} | {status} |\n"
        
        report += f"""
## 4. æ€§èƒ½åˆ†æ

### å‘ç°çš„é—®é¢˜
"""
        
        # åˆ†æé—®é¢˜
        issues = []
        
        if "api_benchmarks" in self.results:
            for result in self.results["api_benchmarks"]:
                if result["avg_time_ms"] > 1000:
                    issues.append(f"- **{result['name']}** å“åº”æ—¶é—´è¿‡é•¿: {result['avg_time_ms']:.2f}ms (è¦æ±‚: â‰¤1000ms)")
                if result["success_rate"] < 80:
                    issues.append(f"- **{result['name']}** æˆåŠŸç‡è¿‡ä½: {result['success_rate']:.1f}% (è¦æ±‚: â‰¥80%)")
        
        if "concurrency_tests" in self.results:
            for result in self.results["concurrency_tests"]:
                if result["success_rate"] < 90:
                    issues.append(f"- **{result['endpoint']}** å¹¶å‘æˆåŠŸç‡è¿‡ä½: {result['success_rate']:.1f}% (è¦æ±‚: â‰¥90%)")
                if result["avg_time_ms"] > 2000:
                    issues.append(f"- **{result['endpoint']}** å¹¶å‘å“åº”æ—¶é—´è¿‡é•¿: {result['avg_time_ms']:.2f}ms")
        
        if "load_tests" in self.results:
            for result in self.results["load_tests"]:
                if result["success_rate"] < 95:
                    issues.append(f"- **{result['scenario_name']}** è´Ÿè½½æµ‹è¯•æˆåŠŸç‡è¿‡ä½: {result['success_rate']:.1f}% (è¦æ±‚: â‰¥95%)")
                if result["avg_response_time_ms"] > 3000:
                    issues.append(f"- **{result['scenario_name']}** è´Ÿè½½æµ‹è¯•å“åº”æ—¶é—´è¿‡é•¿: {result['avg_response_time_ms']:.2f}ms")
        
        if issues:
            for issue in issues:
                report += f"{issue}\n"
        else:
            report += "âœ… æœªå‘ç°ä¸¥é‡æ€§èƒ½é—®é¢˜\n"
        
        report += f"""
### æ€§èƒ½å»ºè®®
"""
        
        # æ€§èƒ½å»ºè®®
        suggestions = []
        
        if "api_benchmarks" in self.results:
            slow_apis = [r for r in self.results["api_benchmarks"] if r["avg_time_ms"] > 800]
            if slow_apis:
                suggestions.append("- **ä¼˜åŒ–æ…¢API**: è€ƒè™‘å¯¹å“åº”æ—¶é—´è¶…è¿‡800msçš„APIè¿›è¡Œä¼˜åŒ–ï¼Œå¦‚æ·»åŠ ç¼“å­˜ã€ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ç­‰")
        
        if "concurrency_tests" in self.results:
            low_concurrency = [r for r in self.results["concurrency_tests"] if r["success_rate"] < 85]
            if low_concurrency:
                suggestions.append("- **æå‡å¹¶å‘èƒ½åŠ›**: å¯¹äºå¹¶å‘æˆåŠŸç‡è¾ƒä½çš„APIï¼Œè€ƒè™‘ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± ã€å¢åŠ æœåŠ¡å™¨èµ„æºç­‰")
        
        if "load_tests" in self.results:
            high_load_issues = [r for r in self.results["load_tests"] if r["success_rate"] < 90 and r["scenario_name"] == "é«˜è´Ÿè½½"]
            if high_load_issues:
                suggestions.append("- **å¢å¼ºé«˜è´Ÿè½½å¤„ç†**: åœ¨é«˜è´Ÿè½½åœºæ™¯ä¸‹ç³»ç»Ÿæ€§èƒ½ä¸‹é™ï¼Œå»ºè®®è¿›è¡Œæ°´å¹³æ‰©å±•æˆ–ä¼˜åŒ–å…³é”®è·¯å¾„")
        
        if not suggestions:
            suggestions.append("- **ä¿æŒå½“å‰ä¼˜åŒ–**: ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®å®šæœŸç›‘æ§æ€§èƒ½æŒ‡æ ‡")
        
        for suggestion in suggestions:
            report += f"{suggestion}\n"
        
        report += f"""
## 5. æµ‹è¯•ç»“è®º

### æ€»ä½“è¯„ä»·
"""
        
        # æ€»ä½“è¯„ä»·
        total_tests = 0
        passed_tests = 0
        
        if "api_benchmarks" in self.results:
            for result in self.results["api_benchmarks"]:
                total_tests += 1
                if result["success_rate"] >= 80 and result["avg_time_ms"] < 1000:
                    passed_tests += 1
        
        if "concurrency_tests" in self.results:
            for result in self.results["concurrency_tests"]:
                total_tests += 1
                if result["success_rate"] >= 90:
                    passed_tests += 1
        
        if "load_tests" in self.results:
            for result in self.results["load_tests"]:
                total_tests += 1
                if result["success_rate"] >= 95:
                    passed_tests += 1
        
        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        if pass_rate >= 90:
            report += "âœ… **ä¼˜ç§€** - ç³»ç»Ÿæ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œæ»¡è¶³æ‰€æœ‰æ€§èƒ½è¦æ±‚\n"
        elif pass_rate >= 70:
            report += "âš ï¸ **è‰¯å¥½** - ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œéƒ¨åˆ†æŒ‡æ ‡éœ€è¦ä¼˜åŒ–\n"
        else:
            report += "âŒ **éœ€è¦æ”¹è¿›** - ç³»ç»Ÿæ€§èƒ½éœ€è¦æ˜¾è‘—æ”¹è¿›\n"
        
        report += f"- **æ€»æµ‹è¯•é¡¹**: {total_tests}\n"
        report += f"- **é€šè¿‡é¡¹**: {passed_tests}\n"
        report += f"- **é€šè¿‡ç‡**: {pass_rate:.1f}%\n"
        
        report += f"""
### åç»­è¡ŒåŠ¨
1. **ç›‘æ§æ€§èƒ½æŒ‡æ ‡**: å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ€§èƒ½ç›‘æ§
2. **å®šæœŸæ€§èƒ½æµ‹è¯•**: å»ºè®®æ¯å‘¨æ‰§è¡Œä¸€æ¬¡æ€§èƒ½æµ‹è¯•
3. **ä¼˜åŒ–æ…¢API**: é’ˆå¯¹å‘ç°çš„æ…¢APIè¿›è¡Œä¼˜åŒ–
4. **å®¹é‡è§„åˆ’**: æ ¹æ®è´Ÿè½½æµ‹è¯•ç»“æœè¿›è¡Œå®¹é‡è§„åˆ’

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**æµ‹è¯•æ‰§è¡Œäºº**: è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print_success(f"æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print_header("å¼€å§‹å…¨é¢æ€§èƒ½æµ‹è¯•")
        
        if not self.login():
            print_error("ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return False
        
        try:
            self.run_api_benchmarks()
            self.run_concurrency_tests()
            self.run_load_tests()
            self.generate_report()
            
            print_header("æ€§èƒ½æµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            print_error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{Colors.BOLD}{Colors.BLUE}{'='*80}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}è£…ä¿®é¿å‘ç®¡å®¶ - å…¨é¢æ€§èƒ½æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{'='*80}{Colors.RESET}\n")
    
    tester = PerformanceTester()
    tester.run_all_tests()

if __name__ == "__main__":
    main()
